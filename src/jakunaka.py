# Paso 1: Importar las librerÃ­as necesarias
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.datasets import mnist
from tensorflow.keras.utils import to_categorical


# Paso 2: Cargar el dataset MNIST
# Contiene imÃ¡genes de dÃ­gitos manuscritos (28x28 pÃ­xeles) y sus etiquetas (0-9).
# recuerda que cada archivo cargado tiene la siguiente estructura
#   Encabezado: Contiene informaciÃ³n sobre cÃ³mo estÃ¡n organizados los datos.
#       Los primeros 16 bytes del archivo son el encabezado y contienen la siguiente informaciÃ³n:   
#
#           Bytes 0-3: El nÃºmero mÃ¡gico (4 bytes) que indica el tipo de archivo. Para imÃ¡genes de MNIST, 
#           este valor es 0x00000801 (32 bits en formato big-endian). Este valor indica que se trata de un archivo de imÃ¡genes.
#           Bytes 4-7: El nÃºmero de elementos en el archivo (nÃºmero de imÃ¡genes). Este valor se almacena en 4 bytes.
#           Bytes 8-11: El nÃºmero de filas de cada imagen (28 para MNIST).
#           Bytes 12-15: El nÃºmero de columnas de cada imagen (28 para MNIST).

#       Por ejemplo el archivo train-images.idx3-ubyte,el primer encabezado serÃ­a algo similar a:
#nÂ° byte    0  1   2    3   4   5   6   7   8   9   10  11  12  13  14  15
#	        00 00  08   03  00  00  ea  60	00  00  00  1c  00  00  00  1c
#       Esto se desglosa como:
#       
#           00 00 08 03 â†’ El nÃºmero mÃ¡gico 0x00000803, que indica que es un archivo de imÃ¡genes.
#           00 00 ea 60 â†’ NÃºmero de imÃ¡genes (por ejemplo, 0xea60 en hexadecimal equivale a 60,000 imÃ¡genes).
#           00 00 00 1c â†’ NÃºmero de filas (28 pÃ­xeles).
#           00 00 00 1c â†’ NÃºmero de columnas (28 pÃ­xeles).

#   Datos de las imÃ¡genes: Los valores de los pÃ­xeles de las imÃ¡genes de los dÃ­gitos manuscritos.
#       DespuÃ©s del encabezado, los datos de las imÃ¡genes siguen. Cada imagen estÃ¡ representada por un 
#       vector de pÃ­xeles de 28x28, es decir, 784 pÃ­xeles (28 * 28). Cada pÃ­xel se almacena como un valor
#       en un byte bajo el sistema hexadecimal, representando la intensidad del pÃ­xel (un valor entre 0
#       y 255, donde 0 es negro y 255 es blanco)
#       
#        Por ejemplo, en los datos que proporcionas:

#nÂ° byte    0   1   2    3   4   5   6   7   8   9   10  11  12  13  14  15
#        	00  00  00  00  00  00  00  00	00  00  00  00  00  00  00  00	
#        	00  00  00  00  00  00  00  00	00  00  00  00  00  00  00  00	
#        	00  00  00  00  00  00  00  00	00  00  00  00  00  00  00  00	
#        	00  00  00  00  00  00  00  00	00  00  00  00  00  00  00  00	
#        	00  00  00  00  00  00  00  00	00  00  00  00  00  00  00  00 
#           ...
#           00  00  00  00  00  00  00  00  03  12  12  12  7e  88  af  1a 
#           ...
#       Cada bloque de 28 bytes despuÃ©s del encabezado corresponderÃ­a a una fila de pÃ­xeles de una imagen. 
#       Todos los valores estÃ¡n en formato hexadecimal.
#
#       Por ejemplo:
#
#       00 o 00 â†’ Esto podrÃ­a representar un pÃ­xel completamente negro (0).
#       03 o 12 â†’ Un valor mÃ¡s alto que indica un pÃ­xel mÃ¡s brillante.
#       7e o 88 â†’ Un valor aÃºn mÃ¡s alto.

#   Una vez que lees los valores de pÃ­xeles, puedes reconstruir la imagen a partir de los 784 pÃ­xeles.
#   Cada imagen de MNIST tiene 28 filas y 28 columnas, y cada valor hexadecimal se convierte a su 
#   valor decimal (entre 0 y 255). Luego puedes representar esos valores como una imagen en una 
#   matriz de 28x28 el cuaL load_data lo separa 50000 muestras de entrenamiento y 100000 muestras 
#   de prueba el cual esta por defecto.

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Paso 3: Normalizar las imÃ¡genes
# Normalizar los valores de los pÃ­xeles para que estÃ©n entre 0 y 1 
# (dividiendo entre 255), o usar una normalizaciÃ³n diferente segÃºn sea necesario
# para el modelo.
x_train = x_train / 255.0  # Normalizar a valores entre 0 y 1
x_test = x_test / 255.0

# Paso 4: Codificar las etiquetas a formato one-hot
# Las etiquetas se convierten a formato one-hot para usar con categorical_crossentropy.
# es una funciÃ³n de pÃ©rdida ampliamente utilizada en problemas de clasificaciÃ³n multiclase en 
# aprendizaje automÃ¡tico, particularmente en redes neuronales. Su propÃ³sito es medir la 
# discrepancia entre las distribuciones de probabilidad predichas por un modelo y las 
# distribuciones reales (etiquetas verdaderas)
#La funciÃ³n de pÃ©rdida de entropÃ­a cruzada compara dos distribuciones de probabilidad:
#   DistribuciÃ³n real (ğ‘¦true): generalmente representada como un vector one-hot que indica la clase correcta.
#   DistribuciÃ³n predicha (ğ‘¦pred): las probabilidades estimadas por el modelo para cada clase.
#   mas info : https://www.lokad.com/es/definicion-de-entropia-cruzada/
y_train = to_categorical(y_train, 10)  # 10 clases (dÃ­gitos del 0 al 9)
y_test = to_categorical(y_test, 10)

# Paso 5: Construir el modelo
#Se define una red con capas densas y una activaciÃ³n softmax para salida categÃ³rica.
#En el modelo presentado, cada neurona tiene un valor de activaciÃ³n que resulta de 
#aplicar una funciÃ³n de activaciÃ³n sobre la suma ponderada de sus entradas (los 
# valores de las neuronas de la capa anterior mÃ¡s un sesgo). A continuaciÃ³n, 
# desglosamos cÃ³mo se calculan estos valores y quÃ© representan en las diferentes capas
# del modelo:
# 1. Capa de entrada (Flatten)
#   Entrada: La imagen 28x28 se aplana en un vector de 784 valores.
#   Valores: Cada valor es la intensidad normalizada de un pÃ­xel (entre 0 y 1, despuÃ©s de dividir por 255).
#   Ejemplo: Si el pÃ­xel tiene un valor de 128 en escala de 8 bits (0-255), 
#   su valor normalizado serÃ¡ 128/255â‰ˆ0.502
# 2. Capa oculta (Dense(128, activation='relu'))
#   CÃ¡lculo: Cada neurona calcula:
#       784
#   z =  âˆ‘ (w[i] . x[i])+b
#       i=1 
#    donde:
#        ğ‘¤[i]    :   Peso asociado a la conexiÃ³n de la entrada 
#        ğ‘¥[ğ‘–]    :   Valor de activaciÃ³n de la capa de entrada (los pÃ­xeles).
#        b       :   Sesgo (bias) aprendido por la red.
#
#    ActivaciÃ³n: Se aplica la funciÃ³n ReLU f(z)=max(0,z)), que pasa los valores positivos sin 
#       cambios y convierte los valores negativos en 0.
#    Valores: DespuÃ©s de la activaciÃ³n, los valores de las neuronas son mayormente positivos o 0.
#   
#   3. Capa de salida (Dense(10, activation='softmax'))
#           CÃ¡lculo: Cada neurona en esta capa calcula:
#                   128
#           ğ‘§[j] =   âˆ‘ (ğ‘¤[ğ‘–][ğ‘—]â‹…ğ‘[ğ‘–])+ğ‘[ğ‘—]
#                   ğ‘–=1
#   
#       donde :
#           ğ‘¤[ğ‘–][ğ‘—],ğ‘[ğ‘–] y ğ‘[ğ‘—]  son anÃ¡logos a los de la capa oculta.
#   
#   ActivaciÃ³n: La funciÃ³n softmax convierte los valores ğ‘§[ğ‘—] en probabilidades:
#                  ğ‘§[ğ‘—]  
#    ğ‘ƒ(ğ‘¦=ğ‘—) =     ğ‘’
#            __________
#              10   ğ‘§[ğ‘˜]
#              âˆ‘   ğ‘’
#              ğ‘˜=1
#
#   Cada neurona de esta capa devuelve una probabilidad asociada a una clase (dÃ­gito 0-9).
#   Valores: Cada valor estÃ¡ entre 0 y 1, y la suma de todos los valores es igual a 1.
#
#    Ejemplo de valores tÃ­picos:
#        Capa de entrada:
#            Valores en el rango [0,1] (intensidad del pÃ­xel).
#        Capa oculta:
#            La mayorÃ­a de las neuronas tienen activaciones > 0 debido a ReLU, pero algunas pueden
#            ser 0 (si ğ‘§ â‰¤ 0).
#        Capa de salida:
#            Ejemplo de salida softmax: [0.1,0.05,0.05,0.8,0.0,0.0,0.0,0.0,0.0,0.0] indicando que el 
#            modelo predice con mayor confianza el dÃ­gito 3.
#
#   En cada paso, los valores evolucionan para capturar patrones jerÃ¡rquicos, desde informaciÃ³n de
#   pÃ­xeles (entrada), caracterÃ­sticas intermedias (oculta) y probabilidades de clase (salida).

#   Los pesos (ğ‘¤) y los sesgos (ğ‘) son los parÃ¡metros aprendidos por una red neuronal durante 
#   el entrenamiento. Estos valores determinan cÃ³mo cada neurona responde a sus entradas. 

#   Â¿QuÃ© son los pesos (w) y los sesgos (ğ‘)?
#   Pesos (ğ‘¤):
#       Representan la importancia de cada conexiÃ³n entre dos neuronas.Son inicializados aleatoriamente
#       al comienzo del entrenamiento y ajustados en cada iteraciÃ³n para minimizar el error del 
#       modelo.Un peso alto significa que la entrada tiene un impacto significativo en la salida
#       de la neurona.
#   
#   Sesgos (b):
#   
#       Son valores adicionales aprendidos por cada neurona para desplazar la salida.
#       Permiten que la neurona modele datos que no estÃ¡n perfectamente centrados en el origen 
#       (es decir, cuando todas las entradas son 0, el sesgo puede hacer que la neurona tenga 
#       un valor distinto de 0).
#   
#   Â¿QuÃ© son ğ‘ğ‘– y ğ‘ğ‘—?
#   ğ‘ğ‘–: Es la salida (o activaciÃ³n) de una neurona ğ‘– en una capa previa.
#   
#   En tÃ©rminos matemÃ¡ticos:
#   
#   
#   ğ‘ğ‘–=ğ‘“(ğ‘§ğ‘–)
#   
#   donde 
#   ğ‘§ğ‘– : es la suma ponderada ( âˆ‘ğ‘¤â‹…ğ‘¥ + ğ‘) y ğ‘“ es la funciÃ³n de activaciÃ³n (ReLU, sigmoid, etc.).
#   ğ‘ğ‘— : Es el sesgo asociado a la neurona ğ‘— en una capa.

#Â¿CÃ³mo se calculan los valores en una capa?
#    Para una capa con ğ‘› neuronas:
#
#1.- Cada neurona ğ‘— calcula una suma ponderada de sus entradas:
#         ğ‘š
#    ğ‘§ğ‘—=  âˆ‘ ğ‘¤ğ‘–ğ‘—â‹…ğ‘ğ‘–+ğ‘ğ‘—
#        ğ‘–=1
#
#    Donde :
#        ğ‘š: NÃºmero de neuronas de la capa anterior.
#        ğ‘¤ğ‘–ğ‘— : Peso entre la neurona ğ‘– de la capa previa y la neurona ğ‘— actual.
#        ğ‘ğ‘– : ActivaciÃ³n de la neurona i de la capa previa.
#        ğ‘ğ‘— : Sesgo de la neurona 
#
#
#Aplica la funciÃ³n de activaciÃ³n (ğ‘“) al valor ğ‘§ğ‘— para obtener ğ‘ğ‘— :
#        ğ‘ğ‘—=ğ‘“(ğ‘§ğ‘—)
#
#Por ejemplo:
#    Para ReLU: ğ‘“(ğ‘§ğ‘—)=maxâ¡(0,ğ‘§ğ‘—) : 
#        La funciÃ³n de activaciÃ³n ReLU (Rectified Linear Unit) es una funciÃ³n que se utiliza
#        en redes neuronales para convertir entradas negativas en cero y emitir directamente las 
#        entradas positivas. Es la funciÃ³n de activaciÃ³n mÃ¡s utilizada en el mundo y
#        se usa en casi todas las redes neuronales convolucionales o de aprendizaje profundo
#    Para softmax: 
#       Se transforma en una probabilidad relativa a las otras salidas.
#       es una funciÃ³n de activaciÃ³n que se utiliza en la capa de salida de una red neuronal
#       para realizar la clasificaciÃ³n multiclase. Esta funciÃ³n se utiliza para convertir las
#       salidas de la capa anterior en probabilidades que suman uno. Las probabilidades se 
#       utilizan para medir la confianza del modelo en la pertenencia a cada clase.

model = Sequential([
    Flatten(input_shape=(28, 28)),  # Aplanar imÃ¡genes 28x28 a un vector de 784
    Dense(128, activation='relu'),  # Capa oculta con 128 neuronas y ReLU
    Dense(10, activation='softmax') # Capa de salida con 10 neuronas (una por clase)
])
#imagen visual de red neuronal : https://panamahitek.com/wp-content/uploads/2023/04/Pasted-9.png
#softmax : https://jacar.es/funcion-softmax-activacion-para-la-clasificacion/
#RELU : https://codificandobits.com/blog/funcion-de-activacion/

# Paso 6: Compilar el modelo
#Â¿CÃ³mo se actualizan los pesos y sesgos?
#El proceso de ajuste ocurre durante el entrenamiento, mediante el algoritmo de retropropagaciÃ³n y 
#un optimizador (como Adam o SGD).
model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# Paso 7: Entrenar el modelo
#   Cuando ejecutas model.fit(), el modelo realiza los siguientes pasos para entrenarse:
#   
#   DivisiÃ³n del conjunto de datos:
#   
#   Se separa el conjunto de datos de entrada en:
#   Conjunto de entrenamiento (80%): Usado para entrenar el modelo.
#   Conjunto de validaciÃ³n (20%): Usado para evaluar el desempeÃ±o despuÃ©s de cada Ã©poca 
#   (sin ajustar los pesos con este conjunto).

#   Forward Propagation (PropagaciÃ³n hacia adelante):
#       Para cada lote (batch), el modelo:
#       Pasa las imÃ¡genes por la red capa a capa.
#       Calcula las predicciones (valores de salida).

#   CÃ¡lculo de la pÃ©rdida (Error):
#      Compara las predicciones con las etiquetas reales (ğ‘¦) usando la funciÃ³n de pÃ©rdida 
#       definida al compilar el modelo (categorical_crossentropy en este caso).
#       Genera un nÃºmero que mide quÃ© tan lejos estÃ¡ el modelo de la respuesta correcta.

#   Backward Propagation (RetropropagaciÃ³n):
#   
#   Calcula los gradientes de la pÃ©rdida respecto a los pesos (ğ‘¤) y sesgos (ğ‘) del modelo 
#   usando el algoritmo de backpropagation.
#  
#   ActualizaciÃ³n de parÃ¡metros:   
#   Usa los gradientes calculados y un optimizador (en este caso, Adam) para ajustar los pesos
#    y sesgos:
#   
#   epochs=5:
#       Es el nÃºmero de veces que el modelo verÃ¡ todo el conjunto de entrenamiento.
#       Una Ã©poca significa que el modelo procesa todos los ejemplos del conjunto de datos una 
#       vez (dividido en lotes).

#   batch_size=32:  
#       Define el nÃºmero de ejemplos que el modelo procesa antes de actualizar los pesos.
#       En este caso, el modelo procesa los datos en grupos de 32 imÃ¡genes (lotes) antes de 
#       ajustar los parÃ¡metros (pesos y sesgos).

#   validation_split=0.2:
#       ProporciÃ³n de los datos de entrenamiento que se reserva para la validaciÃ³n.
#       AquÃ­, el 20% de los datos de x_train y y_train se apartan para validar el desempeÃ±o del
#       modelo en cada Ã©poca (asegurando que no se entrena con estos datos).

model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)

# Paso 8: Evaluar el modelo, Se mide el desempeÃ±o en el conjunto de prueba.
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f'\nPrecisiÃ³n en el conjunto de prueba: {test_acc:.2f}')

# Paso 9: Predecir nuevos valores, predice los dÃ­gitos de imÃ¡genes nuevas.
import numpy as np
predictions = model.predict(x_test[:5])  # Predecir los primeros 5 dÃ­gitos
print("\nPredicciones (nÃºmeros mÃ¡s probables):", np.argmax(predictions, axis=1))
print("Etiquetas reales:", np.argmax(y_test[:5], axis=1))

# Paso 10: Visualizar resultados,Se grafican las imÃ¡genes junto con las predicciones.
import matplotlib.pyplot as plt
for i in range(5):
    plt.imshow(x_test[i], cmap='gray')
    plt.title(f'PredicciÃ³n: {np.argmax(predictions[i])}')
    plt.axis('off')
    plt.show()
